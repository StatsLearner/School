---
title: "Robust Method1"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library("readstata13")
require(ggplot2)
require(tidyverse)
require(moments)
require(normtest)
```

## 1 Skewed data

```{r include=FALSE}
data_gl <- read.dta13("/Users/tetsuroda/Dropbox/LSHTM/Medical Statistics/Term1/Robust Statistics/Datasets for computer practicals-20191115/glucose.dta")
```

### Would the t-test be appropariate?

```{r}
t.test(data_gl$glucose)
```

### Summary
```{r}
summary(data_gl)
```

### Boxplot
```{r}
boxplot(data_gl)
```

### QQ plot
```{r}
qqnorm(data_gl$glucose)
qqline(data_gl$glucose, col = "red", lwd = 2)
```

Based on these plots, the density function is slightly skewed.

### Skewness
```{r message=FALSE}
skewness(data_gl$glucose) # library moments
skewness.norm.test(data_gl$glucose) # library normtest
```

### Conclusion
There is no evidence to reject the null that the skewness = 0. The contradiction between the result of test and visual confirmation is perhaps due to the small sample size. 
All in all, the t-test is inappropriate to perform because of the violation of normality.


## 2 Outliers

```{r include=FALSE}
d <- read.dta13("/Users/tetsuroda/Dropbox/LSHTM/Medical Statistics/Term1/Robust Statistics/Datasets for computer practicals-20191115/ghq.dta")
```

### EDA
```{r}
summary(d)
boxplot(d)
```

### Hypothesis test

Testing to see if the means of each sample is equal to 0.
```{r}
# Assuming the random variables are normal
t.test(d$ghq1)
t.test(d$ghq2)
```

### Are these appropriate in this setting?
The ghq2 can be skewed, therefore, the t-test assuming the normality may be inappropriate. 


### Paired t-test
```{r}
d$diff <- d$ghq1 - d$ghq2
summary(d$diff)
boxplot(d$diff)
hist(d$dif)
t.test(d$diff)

```

### Would this be appropriate?
By the convention of the study, the two variables are paired data. So the paired t-test should be better than non-paired t-test to see if the difference is 0 nor not. 

As the plot shows, in general, the skewness is a little bit improved when taking a difference, although the diff variable is still not normal.

### Correlations

Correlations are susceptible to extreme values. So if some data points away the majority of data are omitted, a correlation coefficient changes.
```{r}
cor(d$ghq1, d$ghq2)
plot(d$ghq1, d$ghq2, type = "p", col = "red", xlab = "ghq1", ylab = "ghq2")

#after removing two data points having ghq1 greater than 20, correlation reduced a bit
cor(d$ghq1[d$ghq1<20], d$ghq2[d$ghq1<20]) 
```


## 3 Relationship between hypothesis testing and CIs

A confidence interval of sample mean difference is obtained by the t-test.
```{r}
t.test(d$diff,mu=2)
```


Chaning the null value and see if the null value starts to be rejected coincides with the confidence interval. In fact, it does.
```{r}

res <- NULL
for (i in seq(from = -1.5, to = 2.2, by = 0.01)){
  t <- t.test(d$diff,mu=i)
  if (t$p.value < 0.05) {
    print(c("rejected, the null value is", i))
  } 
}

```


## 4 P-value simulation
### Two iid random variables following the standard normal distribution
Simulate two iid random variables following the normal distribution and run the t-test for their sample means.

First, create a function to do it.
```{r}
sim_ttest <- function(n,rep,mean,sd){
  vect_p <- NULL
  for (j in 1:rep) {
    x1 <- rnorm(n,mean,sd)
    x2 <- rnorm(n,mean,sd)
    test <- t.test(x1,x2,var.equal = T)
    p <- test$p.value
    vect_p[j] <- p
    }
  return(vect_p)
}
```

Second, run the function.
```{r}
set.seed(1235)
res1 <- sim_ttest(10,1000,0,1)
```


Finally, check a result. The distribution of p-values is the uniform distribution. The proportion of p-values being equal to or less than 0.05 is 4.7%. This is expected because the size of significance level is the size of type 1 error.
```{r message=FALSE}

ggplot(data.frame(pvalue = res1), aes(x=pvalue)) + geom_histogram(binwidth = 0.1)

res1 <- data.frame(pvalue = res1)
p.table <- table(res1$pvalue <= 0.05)
p.table[2]/sum(p.table)

```

### Repeat the simulation changing x2 to another random variable following N(0,5)

```{r}
sim_ttest2 <- function(n,rep,mean,sd){
  vect_p <- NULL
  for (j in 1:rep) {
    x1 <- rnorm(n,mean,sd)
    x2 <- rnorm(n,mean,sd*5)
    test <- t.test(x1,x2,var.equal = T)
    p <- test$p.value
    vect_p[j] <- p
    }
  return(vect_p)
}
```

```{r}
set.seed(1235)
res2 <- sim_ttest2(10,1000,0,1)
```

The distribution of p-values is uniform again. The proportion of p-values being equal to or less than 0.05 becomes 0.069, which is a little bit greater than the expected value, 0.05.
```{r message=FALSE}
ggplot(data.frame(pvalue = res2), aes(x=pvalue)) + geom_histogram(binwidth = 0.1)

res2 <- data.frame(pvalue = res2)
p.table2 <- table(res2$pvalue <= 0.05)
p.table2[2]/sum(p.table2)
```

### Repeat the simulation changing x2 to another random variable following a skewed distribution

```{r}
sim_ttest3 <- function(n,rep,mean,sd){
  vect_p <- NULL
  for (j in 1:rep) {
    x1 <- rnorm(n,mean,sd)
    x2 <- exp(rnorm(n,mean,sd)) - exp(0.5)
    test <- t.test(x1,x2,var.equal = T)
    p <- test$p.value
    vect_p[j] <- p
    }
  return(vect_p)
}
```

```{r}
set.seed(1235)
res3 <- sim_ttest3(10,1000,0,1)
```

The distribution of p-values is not uniform again. The proportion of p-values being equal to or less than 0.05 becomes 0.113, which is greater than the expected value, 0.05.

```{r message=FALSE}
ggplot(data.frame(pvalue = res3), aes(x=pvalue)) + geom_histogram(binwidth = 0.1)

res3 <- data.frame(pvalue = res3)
p.table3 <- table(res3$pvalue <= 0.05)
p.table3[2]/sum(p.table3)
```


### What happens to the simimulation if H0 is not true
Change the x2 to another x2 following N(1,1).

```{r}
sim_ttest4 <- function(n,rep,mean,sd){
  vect_p <- NULL
  for (j in 1:rep) {
    x1 <- rnorm(n,mean,sd)
    x2 <- rnorm(n,mean+1,sd)
    test <- t.test(x1,x2,var.equal = T)
    p <- test$p.value
    vect_p[j] <- p
    }
  return(vect_p)
}
```

```{r}
set.seed(1235)
res4 <- sim_ttest4(10,1000,0,1)
```

The density function of p-values is very skewed and the proportions of p-values equal to or below 0.05 is no more than 55% regardless the H0 is not true.

```{r message=FALSE}
ggplot(data.frame(pvalue = res4), aes(x=pvalue)) + geom_histogram(binwidth = 0.1)

res4 <- data.frame(pvalue = res4)
p.table4 <- table(res4$pvalue <= 0.05)
p.table4[2]/sum(p.table4)
```



### 4-2 P-value simulation - varying sample size
#### Two iid standard normal
Vary the sample sizes, n = 10, n = 100, n = 1000, and do the same simulation.

```{r message=FALSE}
set.seed(13335)
norm <- data.frame(sapply(c(10,100,1000), sim_ttest, 1000, 0,1))
colnames(norm) <- c("n=10", "n=100", "n=1000")

#Wide to long
norm.long <- norm %>%
  gather(samplesize, pvalues, 1:3)

#Histogram
ggplot(norm.long, aes(x=pvalues)) + geom_histogram() +facet_grid(.~samplesize)

#Proportions
#n=10
p.table.norm10 <- table(norm$`n=10` <= 0.05)
n10.norm.res <- p.table.norm10[2]/sum(p.table.norm10)

#n=100
p.table.norm100 <- table(norm$`n=100` <= 0.05)
n100.norm.res <- p.table.norm100[2]/sum(p.table.norm100)

#n=1000
p.table.norm1000 <- table(norm$`n=1000` <= 0.05)
n1000.norm.res <- p.table.norm1000 [2]/sum(p.table.norm1000)

#Print
print(c(paste("n=10:",n10.norm.res), 
        paste("n=100:",n100.norm.res), 
        paste("n=1000:",n1000.norm.res)))
```

Because there is no violiations of assumption, the proportions of p-values being equal to or below 0.05 is close to 0.05.

#### Two normal distributions having differend standard deviations
```{r message=FALSE}
set.seed(13335)
norm <- data.frame(sapply(c(10,100,1000), sim_ttest2, 1000, 0,1))
colnames(norm) <- c("n=10", "n=100", "n=1000")

#Wide to long
norm.long <- norm %>%
  gather(samplesize, pvalues, 1:3)

#Histogram
ggplot(norm.long, aes(x=pvalues)) + geom_histogram() +facet_grid(.~samplesize)

#Proportions
#n=10
p.table.norm10 <- table(norm$`n=10` <= 0.05)
n10.norm.res <- p.table.norm10[2]/sum(p.table.norm10)

#n=100
p.table.norm100 <- table(norm$`n=100` <= 0.05)
n100.norm.res <- p.table.norm100[2]/sum(p.table.norm100)

#n=1000
p.table.norm1000 <- table(norm$`n=1000` <= 0.05)
n1000.norm.res <- p.table.norm1000 [2]/sum(p.table.norm1000)

#Print
print(c(paste("n=10:",n10.norm.res), 
        paste("n=100:",n100.norm.res), 
        paste("n=1000:",n1000.norm.res)))
```

Because the variances are unequal, the smaller the sample size is, the proportions of p-values equal to or less than 0.05 is greather than nominal 0.05, but the differnece is minor.

#### One is normal and another is skewed
```{r message=FALSE}
set.seed(13335)
norm <- data.frame(sapply(c(10,100,1000), sim_ttest3, 1000, 0,1))
colnames(norm) <- c("n=10", "n=100", "n=1000")

#Wide to long
norm.long <- norm %>%
  gather(samplesize, pvalues, 1:3)

#Histogram
ggplot(norm.long, aes(x=pvalues)) + geom_histogram() +facet_grid(.~samplesize)

#Proportions
#n=10
p.table.norm10 <- table(norm$`n=10` <= 0.05)
n10.norm.res <- p.table.norm10[2]/sum(p.table.norm10)

#n=100
p.table.norm100 <- table(norm$`n=100` <= 0.05)
n100.norm.res <- p.table.norm100[2]/sum(p.table.norm100)

#n=1000
p.table.norm1000 <- table(norm$`n=1000` <= 0.05)
n1000.norm.res <- p.table.norm1000 [2]/sum(p.table.norm1000)

#Print
print(c(paste("n=10:",n10.norm.res), 
        paste("n=100:",n100.norm.res), 
        paste("n=1000:",n1000.norm.res)))
```

Because one of the variables are asymmetric, normality assumption is violated. As a result, the proportions of p-values being equal to or less than the nominal value 0.05 are greather, particularly in the case of a small sample size. Nonetheless, when a sample size is large, because of CLT, the proportion converges to 0.05 and the t-test works well.

#### Two normal distributions but population means are not equal

```{r message=FALSE}
set.seed(13335)
norm <- data.frame(sapply(c(10,100,1000), sim_ttest4, 1000, 0,1))
colnames(norm) <- c("n=10", "n=100", "n=1000")

#Wide to long
norm.long <- norm %>%
  gather(samplesize, pvalues, 1:3)

#Histogram
ggplot(norm.long, aes(x=pvalues)) + geom_histogram() +facet_grid(.~samplesize)

#Proportions
#n=10
p.table.norm10 <- table(norm$`n=10` <= 0.05)
n10.norm.res <- p.table.norm10[2]/sum(p.table.norm10)

#n=100
p.table.norm100 <- table(norm$`n=100` <= 0.05)
n100.norm.res <- p.table.norm100[2]/sum(p.table.norm100)
if (is.na(n100.norm.res) == TRUE) {n100.norm.res = 0}

#n=1000
p.table.norm1000 <- table(norm$`n=1000` <= 0.05)
n1000.norm.res <- p.table.norm1000 [2]/sum(p.table.norm1000)
if (is.na(n1000.norm.res) == TRUE) {n1000.norm.res = 0}

#Print
print(c(paste("n=10:",n10.norm.res), 
        paste("n=100:",n100.norm.res), 
        paste("n=1000:",n1000.norm.res)))
```

When the sample size is small, there is a large chance of false negative, which means lacking power.
